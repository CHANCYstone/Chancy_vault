这篇论文是 Alex de Vries 发表在《Joule》（2023）的评论文章，核心结论可以概括为“**AI 的电力消耗正在并将持续快速增长，尤其是推理（inference）阶段的用电需求被长期低估；‘把 AI 无处不在地嵌入’会显著抬高数据中心用电，但最极端情景可能受资源约束所限制；同时，单纯依赖效率提升并不足以抵消总能耗增长，存在明显的反弹效应（rebound effect）**”。主要要点如下：

1. 历史背景与关注点
    

  

- 传统上，数据中心用电约占全球电力的 ~1%，2010–2018 年仅小幅上升约 6%，但生成式 AI 的扩张使“数据中心电力占比可能上冲”的担忧迅速升温。
    

  

2. 训练 vs. 推理：推理可能是更大的“长期消耗端”
    

  

- 文章梳理了大型模型训练的能耗示例（如 BLOOM 训练约 433 MWh，GPT-3 训练约 1,287 MWh），但强调**推理**阶段在生命周期能耗中的份额被低估：例如 Google 报告称其 2019–2021 年与 AI 相关的用电中约 **60% 来自推理**。作者还引用行业测算指出，支持 ChatGPT 规模推理每天可能需要 **~564 MWh**（以 3,617 台 HGX A100 服务器估算），显示推理长期运营的电力“流水出血”。
    

  

3. “AI 上搜索”等大规模应用的能耗外推
    

  

- 若将类似 ChatGPT 的生成式 AI **接入每一次 Google 搜索**，行业估算需要约 **512,821 台 A100 级服务器**，对应 **~80 GWh/日（约 29.2 TWh/年）**的用电量；作者据此指出：若“AI 无处不在”地嵌入互联网关键应用，相关应用本身的电力需求会被显著抬升。
    

  

4. 现实约束与“温和情景”
    

  

- 作者也给出更“务实”的测算路径：以 **NVIDIA 2023 年约 10 万台 AI 服务器出货**为基数（A100/H100 满载约 6.5–10.2 kW/台），对应 **~5.7–8.9 TWh/年**电力需求；并提醒**硬件、资本、供电与冷却**等资源约束在短期内可能**抑制**最极端情景的发生。
    

  

5. “矿二代”与存量 GPU 的迁移效应
    

  

- 以太坊转 PoS 后释放了最多 **9.21 GW** 的算力负荷；若其中 **20% 的 GPU 被转投 AI 工作负载**，“迁移”到 AI 的年用电量可达 **~16.1 TWh**，这会进一步推高 AI 的总体电力需求。
    

  

6. 政策与管理启示
    

  

- **不要走向两极**：既不能过度乐观地认为软硬件效率进步会完全抵消电力增长，也不要在缺乏资源可得性的情况下假定最悲观场景一定发生。效率提升还可能触发**反弹效应**（更高效率→更广应用→总消耗反升）；因此需要在“是否真的需要用 AI”“何种规模/精度合适”与“如何优化系统能效”之间做更审慎的取舍。
    

  

—

**文献信息**：de Vries, A. (2023). _The growing energy footprint of artificial intelligence_. **Joule**, 7(10), 2191–2194. https://doi.org/10.1016/j.joule.2023.09.004 （学术收录与引文信息可见 VU Amsterdam 学术库页面）。

  

如果你要把这些结论写进论文，我可以按你的章节结构（动机→方法→讨论）给出**两段式综述模板**：一段“用能侧约束与反弹效应”，一段“效率侧机遇与边际电源结构”，并配“AI × 能源股连通性”的可检验假设。